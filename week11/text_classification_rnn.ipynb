{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xCosmicx/ATA/blob/main/week11/text_classification_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SN5USFEIIK3"
      },
      "source": [
        "# Text Classification using RNN\n",
        "In this lab exercise, we will learn to use LSTM (an RNN variant) to train a model to classify a piece of text as expressing positive sentiment or negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUQErGewZxE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFctV8-JZOc"
      },
      "source": [
        "### Download the IMDb Dataset\n",
        "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). You will train a sentiment classifier model on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPO4_UmfF0KH",
        "outputId": "ecc057f1-8e2e-4343-f77b-e7ce315dac88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'imdb.vocab', 'imdbEr.txt', 'README', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6yROZNKvbd"
      },
      "source": [
        "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iOHJGN6SDu",
        "outputId": "f5e4220a-e7e2-4dbc-dd63-feb8e4db693d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg',\n",
              " 'urls_neg.txt',\n",
              " 'pos',\n",
              " 'unsupBow.feat',\n",
              " 'labeledBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'unsup',\n",
              " 'urls_unsup.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O59BdioK8jY"
      },
      "source": [
        "The `train` directory also has additional folders which should be removed before creating training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Vfi9oWMSh-"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFoJjiEyJz9u"
      },
      "source": [
        "Next, create a `tf.data.Dataset` using `tf.keras.preprocessing.text_dataset_from_directory`. You can read more about this utility from the [api documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory). \n",
        "\n",
        "Use the `train` directory to create both train and validation datasets with a split of 20% for validation. Also note that here we use a smaller batch size of 128, as our model now is more complex, and will use up some significant memory, leaving little room for larger batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItYD3TLkCOP1",
        "outputId": "5f1335d4-328d-453c-b239-11b68712bad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 128\n",
        "seed = 123\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='training', seed=seed)\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
        "    subset='validation', seed=seed)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa6cq0-Ym0g"
      },
      "source": [
        "Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTCbSkvkYmTT",
        "outputId": "21ae826c-7ef3-4635-9965-cf5c2b5dda4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 b\"I have watched this movie well over 100-200 times, and I love it each and every time I watched it. Yes, it can be very corny but it is also very funny and enjoyable. The camp shown in the movie is a real camp that I actually attended for 7 years and is portrayed as camp really is, a great place to spend the summer. Everyone who has ever gone to camp, wanted to go to camp, or has sent a child to camp should see this movie because it'll bring back wonderful memories for you and for your kids.\"\n",
            "1 b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n",
            "1 b\"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHV2pchDhzDn"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
        "\n",
        "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz6k1IW7h1TO"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGicgV5qT0wh"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6NZSqIIoU0Y"
      },
      "source": [
        "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. \n",
        "\n",
        "TextVectorization layer is a text tokenizer which breaks up the text into words (it is similar to Keras Tokenizer but implemented as a layer). You can read more about TextVectorization layer [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlsXzo-ZlfK"
      },
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "# Use the text vectorization layer to normalize, split, and map strings to \n",
        "# integers.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, \n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsxTnJSv2kF_",
        "outputId": "2e5e7cc6-6547-4ad1-fe90-b7b8b8e73606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(vectorize_layer.get_vocabulary()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI9_wLIiWO8Z"
      },
      "source": [
        "## Create a classification model\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/it3103/bidirectionalRNN.png\"/>\n",
        "\n",
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be built as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the vectorization layer, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the vectorization layer is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHLcFtn5Wsqj"
      },
      "source": [
        "EMBEDDING_DIM=128\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
        "              output_dim=EMBEDDING_DIM, \n",
        "              mask_zero=False, \n",
        "              name='embedding'),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLNgKO7W2fe"
      },
      "source": [
        "## Compile and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpX9etB6IOQd"
      },
      "source": [
        "You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Hg3IHFt4Px"
      },
      "source": [
        "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "def get_run_logdir():    # use a new directory for each run\n",
        "    import time\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_logdir)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"bestcheckpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrKAKAKIbuH"
      },
      "source": [
        "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCUgdP69Wzix"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mQehiQyv8rP",
        "outputId": "9ba667e8-e31b-4fbe-cf9c-645baf76bb4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(\n",
        "    train_ds, \n",
        "    validation_data=val_ds,\n",
        "    epochs=5, \n",
        "    callbacks=[tensorboard_callback, model_checkpoint_callback])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 15s 56ms/step - loss: 0.4447 - accuracy: 0.7811 - val_loss: 0.3379 - val_accuracy: 0.8594\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 6s 41ms/step - loss: 0.2579 - accuracy: 0.8991 - val_loss: 0.4295 - val_accuracy: 0.8430\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 7s 42ms/step - loss: 0.2279 - accuracy: 0.9122 - val_loss: 0.5411 - val_accuracy: 0.8422\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 6s 41ms/step - loss: 0.1937 - accuracy: 0.9248 - val_loss: 0.4038 - val_accuracy: 0.8360\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 6s 41ms/step - loss: 0.1541 - accuracy: 0.9428 - val_loss: 0.4380 - val_accuracy: 0.8350\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0dd60f3510>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQbOJZ2WBFY"
      },
      "source": [
        "Visualize the model metrics in TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uanp2YH8RzU",
        "outputId": "b535615b-99ab-4225-815a-178e61e16ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 235), started 1:06:43 ago. (Use '!kill 235' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wYnVedSPfmX"
      },
      "source": [
        "The model reaches a validation accuracy of around 85% after 1 epoch of training.\n",
        "\n",
        "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTJMGvvR6mTs"
      },
      "source": [
        "Let's evaluate the model on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30fq4P6C5aI8",
        "outputId": "70a4f1ee-a28e-48b2-eac2-c7f017dac119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=128)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk-tEK086qko",
        "outputId": "45968175-d239-40bc-a17e-d1aaf7a9da78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.evaluate(test_ds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 11s 53ms/step - loss: 0.4993 - accuracy: 0.8142\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.49925389885902405, 0.8141599893569946]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMSXXTSZ8eh_"
      },
      "source": [
        "Here we show how we can use get all the individual predictions for the test_ds and use the predictions to plot the confusion_matrix and classification report to allow us to have better insight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrf21sRgWTfK"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_preds = np.array([])\n",
        "y_labels = np.array([])\n",
        "count = 0\n",
        "for texts, labels in test_ds:\n",
        "    preds = model.predict(texts)\n",
        "    preds = (preds >= 0.5).reshape(-1)\n",
        "    y_preds = np.concatenate((y_preds, preds), axis=0)\n",
        "    y_labels = np.concatenate((y_labels, labels), axis=0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxh8YMp5cTf4",
        "outputId": "d6f4cbdb-11a7-4e22-bc4d-c8449e532ecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_labels, y_preds))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.79      0.81     12500\n",
            "         1.0       0.80      0.84      0.82     12500\n",
            "\n",
            "    accuracy                           0.81     25000\n",
            "   macro avg       0.81      0.81      0.81     25000\n",
            "weighted avg       0.81      0.81      0.81     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzSEgryN1_1B"
      },
      "source": [
        "\n",
        "Let's go ahead and save our model. You will see that our model achieve an accuracy of around 82%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnKLmU_ksqk",
        "outputId": "13ee6b00-5895-40e8-9860-de624cb9fb30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.save('sentiment_model')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: sentiment_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: sentiment_model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f0dd60dc710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f0dd1b93850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtek2H0A2TWK"
      },
      "source": [
        "Now let us put our model in use!!  We will first load our saved model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1DJOzal2yQl"
      },
      "source": [
        "loaded_model = tf.keras.models.load_model('sentiment_model')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edrL9YQ03Tqb",
        "outputId": "201a4a97-0ae1-4fea-fb07-861dd238ff0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 200)              0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 128)          1280000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,387,137\n",
            "Trainable params: 1,387,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLaU3AKb3T7v"
      },
      "source": [
        "Run the following cell and type in your own text at the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBr_A1W2h6F",
        "outputId": "366544f8-f272-44d9-a9a1-c9463e0b157e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = input(\"Write your review here:\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write your review here:i hate azfar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR-ZsF345rSU",
        "outputId": "7fccd958-5f8d-4b20-8791-2ba68cac3ce1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred = loaded_model.predict([text])[0]\n",
        "if pred >= 0.5: \n",
        "    print('positive sentiment')\n",
        "else:\n",
        "    print('negative sentiment')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c570Kdh-kmYZ"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/it3103/layered_bidirectional.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29uXYt5gkuW8"
      },
      "source": [
        "VOCAB_SIZE = 100000\n",
        "EMBEDDDING_DIM = 128\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM),\n",
        "    # tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.4, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.3)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYIpOKjFk2SU"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YluqzM-kk29d",
        "outputId": "e58d5b34-b846-42a0-a043-9b2cafff59ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(train_ds, \n",
        "          epochs=5,\n",
        "          validation_data=val_ds,\n",
        "          callbacks=[tensorboard_callback, model_checkpoint_callback])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 22s 84ms/step - loss: 0.6920 - accuracy: 0.5354 - val_loss: 0.6861 - val_accuracy: 0.6230\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.5174 - accuracy: 0.7569 - val_loss: 0.3734 - val_accuracy: 0.8468\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.3430 - accuracy: 0.8636 - val_loss: 0.3340 - val_accuracy: 0.8580\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.2838 - accuracy: 0.8925 - val_loss: 0.3283 - val_accuracy: 0.8612\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.2510 - accuracy: 0.9075 - val_loss: 0.3407 - val_accuracy: 0.8598\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0cc1375c50>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tDbhq4ek9lq",
        "outputId": "8ee761ec-bb7f-4dd9-805a-623e954e9644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196/196 [==============================] - 9s 46ms/step - loss: 0.3665 - accuracy: 0.8500\n",
            "Test Loss: 0.36652952432632446\n",
            "Test Accuracy: 0.8500000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrUj22o7lDhc",
        "outputId": "ca0cd2df-9a65-4854-b71c-3d046ffabda7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_text = 'the movie is such a joy to watch'\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print( 'positive' if predictions >= 0.5 else 'negative')\n",
        "print(predictions)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n",
            "[[0.4822376]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZ8iX8AqtF2"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Experiment with any of following to see if you get better or worse validation accuracy.\n",
        "\n",
        "1. Add in Dropout layer\n",
        "2. Increase vocabulary size \n",
        "2. Increase Embedding dimensions \n",
        "4. Use uni-directional LSTM instead of bidirectional\n"
      ]
    }
  ]
}