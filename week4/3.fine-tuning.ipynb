{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xCosmicx/ATA/blob/main/week4/3.fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce6b8183-8ef0-4664-813f-6a635bda0f15",
      "metadata": {
        "id": "ce6b8183-8ef0-4664-813f-6a635bda0f15"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Another widely used transfer learning technique is _fine-tuning_. \n",
        "Fine-tuning involves unfreezing a few of the top layers \n",
        "of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in our case, the \n",
        "fully-connected classifier) and these unfrozen top layers. This is called \"fine-tuning\" because it slightly adjusts the more abstract \n",
        "representations of the model being reused, in order to make them more relevant for the problem at hand.\n",
        "\n",
        "![fine-tuning VGG16](https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/resources/vgg16_fine_tuning.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "768ead86-19a3-4908-be5b-57538c555a19",
      "metadata": {
        "id": "768ead86-19a3-4908-be5b-57538c555a19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790141c4-2bee-4431-8449-204417495a64",
      "metadata": {
        "id": "790141c4-2bee-4431-8449-204417495a64"
      },
      "source": [
        "It was necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized \n",
        "classifier on top. For the same reason, it is only possible to fine-tune the top layers of the convolutional base once the classifier on \n",
        "top has already been trained. If the classified wasn't already trained, then the error signal propagating through the network during \n",
        "training would be too large, and the representations previously learned by the layers being fine-tuned would be destroyed. Thus the steps \n",
        "for fine-tuning a network are as follow:\n",
        "\n",
        "1. Add your custom network on top of an already trained base network.\n",
        "2. Freeze the base network.\n",
        "3. Train the part you added.\n",
        "4. Unfreeze some layers in the base network.\n",
        "5. Jointly train both these layers and the part you added.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "RB9VSinUyvhV",
      "metadata": {
        "id": "RB9VSinUyvhV",
        "outputId": "ea1224b3-0e1f-4ea8-d7ee-1ea8ed24df84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "img_height, img_width = 128, 128\n",
        "\n",
        "# Load the pre-trained model \n",
        "base_model = keras.applications.VGG16(input_shape=(img_height, img_width) + (3,),\n",
        "                                         include_top=False,\n",
        "                                         weights='imagenet')\n",
        "\n",
        "preprocess_input_fn = keras.applications.vgg16.preprocess_input\n",
        "\n",
        "# freeze the base layer \n",
        "base_model.trainable = False\n",
        "\n",
        "# Add input layer \n",
        "inputs = layers.Input(shape=(img_height, img_width, 3))\n",
        "# Add preprocessing layer\n",
        "x = preprocess_input_fn(inputs)\n",
        "# Add the base, set training to false to freeze the convolutional base\n",
        "x = base_model(x)\n",
        "# Add our classification head\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(rate=0.5)(x)\n",
        "x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "x = layers.Dropout(rate=0.5)(x)\n",
        "outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=base_learning_rate), \n",
        "                  metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0UVNujET2sho",
      "metadata": {
        "id": "0UVNujET2sho"
      },
      "source": [
        "Let's confirm all the layers of convolutional base are frozen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NX2qOF-52eVq",
      "metadata": {
        "id": "NX2qOF-52eVq",
        "outputId": "08fb7b5b-376e-4a81-da2e-13a71167a8bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_1 False\n",
            "block1_conv1 False\n",
            "block1_conv2 False\n",
            "block1_pool False\n",
            "block2_conv1 False\n",
            "block2_conv2 False\n",
            "block2_pool False\n",
            "block3_conv1 False\n",
            "block3_conv2 False\n",
            "block3_conv3 False\n",
            "block3_pool False\n",
            "block4_conv1 False\n",
            "block4_conv2 False\n",
            "block4_conv3 False\n",
            "block4_pool False\n",
            "block5_conv1 False\n",
            "block5_conv2 False\n",
            "block5_conv3 False\n",
            "block5_pool False\n"
          ]
        }
      ],
      "source": [
        "for layer in base_model.layers:\n",
        "    print(layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q418z8cI3V-O",
      "metadata": {
        "id": "Q418z8cI3V-O"
      },
      "source": [
        "Let's print out the model summary and see how many trainable weights. We can see that we only 263,169 trainable weights (parameters), coming from the classification head that put on top of the convolutional base. (For comparison, a VGG16 has total of 14,714,688 weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ZkwKVe2_2_qF",
      "metadata": {
        "id": "ZkwKVe2_2_qF",
        "outputId": "08e4aa7c-0fb4-4e9c-c770-aa4071f562ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 128, 128, 3)      0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            " tf.nn.bias_add (TFOpLambda)  (None, 128, 128, 3)      0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,977,857\n",
            "Trainable params: 263,169\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "toPgcpoW4HIm",
      "metadata": {
        "id": "toPgcpoW4HIm"
      },
      "source": [
        "## Creating Datasets\n",
        "\n",
        "We will setup our training and validation dataset as we did in earlier exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dc8d2e8a-b80a-410a-afb7-b7534a83af77",
      "metadata": {
        "id": "dc8d2e8a-b80a-410a-afb7-b7534a83af77",
        "outputId": "e6022aa9-19c1-4445-d97a-31cd3214b82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz\n",
            "67043328/67041740 [==============================] - 5s 0us/step\n",
            "67051520/67041740 [==============================] - 5s 0us/step\n"
          ]
        }
      ],
      "source": [
        "dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs_subset.tar.gz', origin=dataset_URL, extract=True, cache_dir='.')\n",
        "dataset_dir = os.path.join(os.path.dirname(path_to_zip), \"cats_and_dogs_subset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "208febe1-3fdf-41e3-999c-3fac7e70f007",
      "metadata": {
        "id": "208febe1-3fdf-41e3-999c-3fac7e70f007",
        "outputId": "244c7854-aeb5-43b8-c695-fce1a1f9bb96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 files belonging to 2 classes.\n",
            "Using 2400 files for training.\n",
            "Found 3000 files belonging to 2 classes.\n",
            "Using 600 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "image_size = (img_height, img_width)\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZSyhl8et47AV",
      "metadata": {
        "id": "ZSyhl8et47AV"
      },
      "source": [
        "## Train the classification head \n",
        "\n",
        "We will go ahead and train our classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cqSDQ4gP5HEO",
      "metadata": {
        "id": "cqSDQ4gP5HEO",
        "outputId": "ac545fbb-b0ed-4625-9895-c259cbc20606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "75/75 [==============================] - 19s 87ms/step - loss: 1.8639 - accuracy: 0.8517 - val_loss: 0.4198 - val_accuracy: 0.9417\n",
            "Epoch 2/30\n",
            "75/75 [==============================] - 6s 73ms/step - loss: 0.8488 - accuracy: 0.9104 - val_loss: 0.4033 - val_accuracy: 0.9450\n",
            "Epoch 3/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.7672 - accuracy: 0.9175 - val_loss: 0.3106 - val_accuracy: 0.9367\n",
            "Epoch 4/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.5993 - accuracy: 0.9183 - val_loss: 0.2460 - val_accuracy: 0.9483\n",
            "Epoch 5/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.4176 - accuracy: 0.9258 - val_loss: 0.2281 - val_accuracy: 0.9400\n",
            "Epoch 6/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.3037 - accuracy: 0.9308 - val_loss: 0.2300 - val_accuracy: 0.9417\n",
            "Epoch 7/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.3084 - accuracy: 0.9304 - val_loss: 0.1696 - val_accuracy: 0.9433\n",
            "Epoch 8/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.2181 - accuracy: 0.9350 - val_loss: 0.1947 - val_accuracy: 0.9333\n",
            "Epoch 9/30\n",
            "75/75 [==============================] - 5s 71ms/step - loss: 0.2037 - accuracy: 0.9371 - val_loss: 0.1593 - val_accuracy: 0.9450\n",
            "Epoch 10/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.2078 - accuracy: 0.9362 - val_loss: 0.1566 - val_accuracy: 0.9417\n",
            "Epoch 11/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1678 - accuracy: 0.9442 - val_loss: 0.1489 - val_accuracy: 0.9417\n",
            "Epoch 12/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1593 - accuracy: 0.9458 - val_loss: 0.1481 - val_accuracy: 0.9450\n",
            "Epoch 13/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1292 - accuracy: 0.9504 - val_loss: 0.1654 - val_accuracy: 0.9400\n",
            "Epoch 14/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1736 - accuracy: 0.9383 - val_loss: 0.1626 - val_accuracy: 0.9383\n",
            "Epoch 15/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1407 - accuracy: 0.9488 - val_loss: 0.1591 - val_accuracy: 0.9483\n",
            "Epoch 16/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1213 - accuracy: 0.9525 - val_loss: 0.1773 - val_accuracy: 0.9267\n",
            "Epoch 17/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1395 - accuracy: 0.9488 - val_loss: 0.1561 - val_accuracy: 0.9367\n",
            "Epoch 18/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1208 - accuracy: 0.9563 - val_loss: 0.1541 - val_accuracy: 0.9333\n",
            "Epoch 19/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1307 - accuracy: 0.9513 - val_loss: 0.1580 - val_accuracy: 0.9317\n",
            "Epoch 20/30\n",
            "75/75 [==============================] - 6s 72ms/step - loss: 0.1198 - accuracy: 0.9579 - val_loss: 0.1390 - val_accuracy: 0.9500\n",
            "Epoch 21/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1280 - accuracy: 0.9479 - val_loss: 0.1447 - val_accuracy: 0.9433\n",
            "Epoch 22/30\n",
            "75/75 [==============================] - 6s 70ms/step - loss: 0.1059 - accuracy: 0.9613 - val_loss: 0.1426 - val_accuracy: 0.9500\n",
            "Epoch 23/30\n",
            "75/75 [==============================] - 6s 73ms/step - loss: 0.1169 - accuracy: 0.9558 - val_loss: 0.1337 - val_accuracy: 0.9533\n",
            "Epoch 24/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1186 - accuracy: 0.9567 - val_loss: 0.1374 - val_accuracy: 0.9500\n",
            "Epoch 25/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1072 - accuracy: 0.9579 - val_loss: 0.1471 - val_accuracy: 0.9500\n",
            "Epoch 26/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1156 - accuracy: 0.9579 - val_loss: 0.1411 - val_accuracy: 0.9500\n",
            "Epoch 27/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1058 - accuracy: 0.9588 - val_loss: 0.1403 - val_accuracy: 0.9483\n",
            "Epoch 28/30\n",
            "75/75 [==============================] - 5s 70ms/step - loss: 0.1161 - accuracy: 0.9588 - val_loss: 0.1330 - val_accuracy: 0.9517\n",
            "Epoch 29/30\n",
            "75/75 [==============================] - 5s 71ms/step - loss: 0.0944 - accuracy: 0.9658 - val_loss: 0.1453 - val_accuracy: 0.9500\n",
            "Epoch 30/30\n",
            "75/75 [==============================] - 6s 71ms/step - loss: 0.1111 - accuracy: 0.9592 - val_loss: 0.1441 - val_accuracy: 0.9533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f14f003ec90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# create model checkpoint callback to save the best model checkpoint\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, \n",
        "          epochs=30, callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172d426c-7666-47b1-84a6-485826b79641",
      "metadata": {
        "id": "172d426c-7666-47b1-84a6-485826b79641"
      },
      "source": [
        "Now we have our classification layers trained, let's start to unfreeze some top layers of the convolutional base to fine tune the weights. \n",
        "We will fine-tune the last 3 convolutional layers, which means that all layers up until `block4_pool` should be frozen, and the layers \n",
        "`block5_conv1`, `block5_conv2` and `block5_conv3` should be trainable.\n",
        "\n",
        "Why not fine-tune more layers? Why not fine-tune the entire convolutional base? We could. However, we need to consider that:\n",
        "\n",
        "* Earlier layers in the convolutional base encode more generic, reusable features, while layers higher up encode more specialized features. It is \n",
        "more useful to fine-tune the more specialized features, as these are the ones that need to be repurposed on our new problem. There would \n",
        "be fast-decreasing returns in fine-tuning lower layers.\n",
        "* The more parameters we are training, the more we are at risk of overfitting. The convolutional base has 15M parameters, so it would be \n",
        "risky to attempt to train it on our small dataset.\n",
        "\n",
        "Thus, in our situation, it is a good strategy to only fine-tune the top 2 to 3 layers in the convolutional base.\n",
        "\n",
        "Let's set this up, we will unfreeze our `base_model`, \n",
        "and then freeze individual layers inside of it, except the last 3 layers. \n",
        "\n",
        "Do a model ``summary()`` and you will see now that the number of trainable weights are now 7,079,424 (around 7 millions), much less than previously, because all the layers are frozen except the last 3 layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "690283a8-4c53-4293-b8f2-c0685d8031aa",
      "metadata": {
        "id": "690283a8-4c53-4293-b8f2-c0685d8031aa"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wWXyo3n5903l",
      "metadata": {
        "id": "wWXyo3n5903l"
      },
      "source": [
        "Let us examine model summary again. We can see now that we have more trainable weights 7,342,593 compared to previously 263,169."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8uq86nMv-HxP",
      "metadata": {
        "id": "8uq86nMv-HxP",
        "outputId": "7bbbcfad-a589-4d94-c38b-8749c826d647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 128, 128, 3)      0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            " tf.nn.bias_add (TFOpLambda)  (None, 128, 128, 3)      0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,977,857\n",
            "Trainable params: 7,342,593\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdZPk1Ay80r_",
      "metadata": {
        "id": "bdZPk1Ay80r_"
      },
      "source": [
        "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage as we do not want to make too drastic changes to the weights in the convolutional layers under fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bc1da460-3c8a-40f3-a9e5-14c7b832ef79",
      "metadata": {
        "id": "bc1da460-3c8a-40f3-a9e5-14c7b832ef79",
        "outputId": "80cdeb39-78cb-4214-d35c-2bd98896e178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "75/75 [==============================] - 8s 80ms/step - loss: 0.2225 - accuracy: 0.9262 - val_loss: 0.1342 - val_accuracy: 0.9533\n",
            "Epoch 2/15\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 0.1009 - accuracy: 0.9658 - val_loss: 0.1213 - val_accuracy: 0.9467\n",
            "Epoch 3/15\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 0.0592 - accuracy: 0.9787 - val_loss: 0.1549 - val_accuracy: 0.9383\n",
            "Epoch 4/15\n",
            "75/75 [==============================] - 6s 82ms/step - loss: 0.0312 - accuracy: 0.9892 - val_loss: 0.1158 - val_accuracy: 0.9583\n",
            "Epoch 5/15\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 0.0330 - accuracy: 0.9858 - val_loss: 0.1672 - val_accuracy: 0.9483\n",
            "Epoch 6/15\n",
            "75/75 [==============================] - 6s 79ms/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.1589 - val_accuracy: 0.9517\n",
            "Epoch 7/15\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.1842 - val_accuracy: 0.9450\n",
            "Epoch 8/15\n",
            "75/75 [==============================] - 6s 79ms/step - loss: 0.0340 - accuracy: 0.9900 - val_loss: 0.1220 - val_accuracy: 0.9550\n",
            "Epoch 9/15\n",
            "75/75 [==============================] - 6s 78ms/step - loss: 0.0144 - accuracy: 0.9971 - val_loss: 0.1550 - val_accuracy: 0.9500\n",
            "Epoch 10/15\n",
            "75/75 [==============================] - 6s 79ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9517\n",
            "Epoch 11/15\n",
            "75/75 [==============================] - 6s 79ms/step - loss: 0.0067 - accuracy: 0.9975 - val_loss: 0.2064 - val_accuracy: 0.9517\n",
            "Epoch 12/15\n",
            "75/75 [==============================] - 6s 79ms/step - loss: 0.0114 - accuracy: 0.9950 - val_loss: 0.3346 - val_accuracy: 0.9300\n",
            "Epoch 13/15\n",
            "75/75 [==============================] - 6s 79ms/step - loss: 0.0247 - accuracy: 0.9921 - val_loss: 0.3054 - val_accuracy: 0.9467\n",
            "Epoch 14/15\n",
            "75/75 [==============================] - 6s 80ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.1819 - val_accuracy: 0.9533\n",
            "Epoch 15/15\n",
            "75/75 [==============================] - 6s 80ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.1893 - val_accuracy: 0.9533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f147c066b50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "finetune_learning_rate = base_learning_rate / 10.\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=finetune_learning_rate),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=15,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2496b41f-63bd-42ca-aa6a-923a6588d4d8",
      "metadata": {
        "id": "2496b41f-63bd-42ca-aa6a-923a6588d4d8",
        "outputId": "8b081868-2dfe-4d98-a0ba-12657c1cdbb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 1s 56ms/step - loss: 0.1158 - accuracy: 0.9583\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11575710028409958, 0.9583333134651184]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.load_weights('best_checkpoint')\n",
        "model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RKt9lTt3_qYi",
      "metadata": {
        "id": "RKt9lTt3_qYi"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "1. Is our fine-tuned model performing better or worse? \n",
        "2. Try to unfreeze less/more layers and see if the model performance improves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R1QvEPcjwvs0",
      "metadata": {
        "id": "R1QvEPcjwvs0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3.fine-tuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}